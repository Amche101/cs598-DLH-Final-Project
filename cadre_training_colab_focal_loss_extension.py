# -*- coding: utf-8 -*-
"""CADRE_cv_training_colab_dlh.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UvYh1GB65DDzi_JV5vuo_hLNtG3twwBZ
"""

# Install dependencies if running in Colab
#!pip install torch pandas numpy scikit-learn matplotlib

!pwd
!ls /content/

!date

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.autograd import Variable
from sklearn.model_selection import train_test_split
from sklearn.metrics import f1_score, accuracy_score, roc_auc_score, log_loss


# Small epsilon value to prevent division by zero
EPSILON = 1e-5

device = 'cuda' if torch.cuda.is_available() else 'cpu'
print("device = ",device)

##############################################################################
# Preprocessing function (adapted from original CADRE paper with simplifications)
##############################################################################
def preprocess_cadre_data(input_dir, repository="gdsc"):
    tgt = pd.read_csv(f"{input_dir}/{repository}.csv", index_col=0)
    drug_info = pd.read_csv(f"{input_dir}/drug_info_{repository}.csv", index_col=0)
    omics_data = {omic: pd.read_csv(f"{input_dir}/{omic}_{repository}.csv", index_col=0)
                  for omic in ['mut', 'cnv', 'exp', 'met']}

    common_samples = set(tgt.index)
    for df in omics_data.values():
        common_samples &= set(df.index)
    common_samples = sorted(list(common_samples))

    tgt = tgt.loc[common_samples].fillna(0).astype(int)
    mask = tgt.notnull().astype(int).values
    data = {"tgt": tgt.values, "msk": mask, "tmr": common_samples}

    for omic, df in omics_data.items():
        mat = df.loc[common_samples].values.astype(int)
        data[f"{omic}_bin"] = mat
        idx_matrix = np.zeros((mat.shape[0], mat.sum(axis=1).max()), dtype=int)
        for i, row in enumerate(mat):
            idx_matrix[i, :sum(row)] = np.where(row == 1)[0] + 1
        data[f"{omic}_idx"] = idx_matrix
    return data
  
##############################################################################
# Simple collaborative filtering model with contextual attention (inspired by CADRE)
##############################################################################
class ContextualAttentionCF(nn.Module):
    def __init__(self, omic_dim, drug_dim, hidden_dim=192, dropout=0.4):
        super().__init__()
        self.omic_encoder = nn.Sequential(
            nn.Linear(omic_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout)
        )
        self.drug_embed = nn.Embedding(drug_dim, hidden_dim)
        self.output_layer = nn.Linear(hidden_dim, 1)

    def forward(self, x_omic, drug_ids):
        omic_feat = self.omic_encoder(x_omic)
        drug_feat = self.drug_embed(drug_ids)
        combined = omic_feat * drug_feat  # Element-wise interaction
        return torch.sigmoid(self.output_layer(combined)).squeeze()

"""
class DrugTargetEmbeddingCF(nn.Module):
    def __init__(self, omic_dim, drug_target_dim, hidden_dim=128, dropout=0.5):
        super().__init__()

        # Encode omics (e.g., gene expression)
        self.omic_encoder = nn.Sequential(
            nn.Linear(omic_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout)
        )

        # Encode drug target vector
        self.drug_encoder = nn.Sequential(
            nn.Linear(drug_target_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout)
        )

        # Final prediction layer
        self.output_layer = nn.Linear(hidden_dim, 1)

    def forward(self, x_omic, x_drug_target):
        omic_feat = self.omic_encoder(x_omic)
        drug_feat = self.drug_encoder(x_drug_target)
        combined = omic_feat * drug_feat  # element-wise multiplication
        return torch.sigmoid(self.output_layer(combined)).squeeze()
"""
##############################################################################
# extension : new loss function: Focal Loss, useful for data with significant imbalance
##############################################################################
class FocalLoss(nn.Module):
    """
    Focal Loss for binary classification.

    Args:
        gamma (float, optional): Focusing parameter. Defaults to 2.0.
        alpha (float, optional): Weighting factor for the positive class. Defaults to 0.25.
        reduction (str, optional): Specifies the reduction to apply to the
            output: 'none' | 'mean' | 'sum'.  'none': no reduction will be applied,
            'mean': the sum of the output will be divided by the number of
            elements in the output, 'sum': the output will be summed. Default: 'mean'
    """

    def __init__(self, gamma: float = 2.0, alpha: float = 0.25, reduction: str = 'mean') -> None: 
        super().__init__()
        self.gamma = gamma
        self.alpha = alpha
        self.reduction = reduction

    def forward(self, input: torch.Tensor, target: torch.Tensor) -> torch.Tensor:
        """
        Forward pass of the Focal Loss.

        Args:
            input (torch.Tensor): Predicted probabilities (batch_size,).
            target (torch.Tensor): Ground truth labels (batch_size,).

        Returns:
            torch.Tensor: Computed Focal Loss.
        """
        # Ensure input is in the probability range (0, 1)
        input = torch.clamp(input, 1e-7, 1 - 1e-7)

        # Calculate cross-entropy loss
        cross_entropy_loss = F.cross_entropy(input, target, reduction='none')

        pt=torch.exp(-cross_entropy_loss)

        # Calculate the modulating factor
        modulating_factor = (1 - pt).pow(self.gamma)

        # Apply alpha weighting
        alpha_weight =  self.alpha * modulating_factor

        # Calculate focal loss
        focal_loss = alpha_weight * cross_entropy_loss

        if self.reduction == 'mean':
            return torch.mean(focal_loss)
        elif self.reduction == 'sum':
            return torch.sum(focal_loss)
        else:
            return focal_loss
#######################################
# Training and evaluation logic
#######################################
def train_model(model, data_loader, optimizer, criterion):
    #if torch.cuda.is_available():
        #print("using cuda")
        #print(model.device)
        #model = model.to(device)
        #criterion = criterion.to(device)

    model.train()
    total_loss = 0
    for x, y, d in data_loader:
        if torch.cuda.is_available():
            x = x.to(device)
            y = y.to(device)
            d = d.to(device)
        optimizer.zero_grad()
        preds = model(x, d)
        preds = preds.to(device)
        loss = criterion(preds, y.float())
        loss = loss.to(device)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    return total_loss / len(data_loader)

def evaluate_model(model, data_loader):
    #if torch.cuda.is_available():
    #    model = model.to(device)
    model.eval()
    preds, targets = [], []
    with torch.no_grad():
        for x, y, d in data_loader:
            if torch.cuda.is_available():
                x = x.to(device)
                y = y.to(device)
                d = d.to(device)
            outputs = model(x, d)
            outputs = outputs.to('cpu')
            y = y.to('cpu')
            preds.extend(outputs.numpy())
            targets.extend(y.numpy())
    preds_binary = [1 if p > 0.5 else 0 for p in preds]
    return {
        'f1': f1_score(targets, preds_binary),
        'accuracy': accuracy_score(targets, preds_binary),
        'roc_auc': roc_auc_score(targets, preds)
    }

##########################################
# data procesing
##########################################
input_dir = "data"
#input_dir = "/content"
#input_dir = "/content/gdrive/My Drive/CADRE-master/data/input"
#input_dir = "/content/data/input"
repository = "gdsc"  # or 'ccle'

data = preprocess_cadre_data(input_dir=input_dir, repository=repository)

# Example: Using gene expression (exp_bin) as input features
X = torch.FloatTensor(data['exp_bin'])
Y = torch.FloatTensor(data['tgt'])
mask = torch.FloatTensor(data['msk'])
drug_ids = torch.arange(Y.shape[1]).repeat(X.shape[0], 1)

# Flatten to simulate a sample per (cell line, drug) pair
X_flat = X.repeat_interleave(Y.shape[1], dim=0)
drug_ids_flat = drug_ids.flatten().long()
Y_flat = Y.flatten()
mask_flat = mask.flatten()

# Keep only labeled entries
valid_idx = mask_flat == 1
X_train = X_flat[valid_idx]
Y_train = Y_flat[valid_idx]
D_train = drug_ids_flat[valid_idx]

##########################################
#  train for 5 epochs
##########################################
from torch.utils.data import DataLoader, TensorDataset

# Wrap into DataLoader
dataset = TensorDataset(X_train, Y_train, D_train)
train_loader = DataLoader(dataset, batch_size=64, shuffle=True)

# Initialize model, optimizer, and loss
model = ContextualAttentionCF(omic_dim=X.shape[1], drug_dim=Y.shape[1])
model = model.to(device)
optimizer = optim.Adam(model.parameters(), lr=0.001)
#criterion = nn.BCELoss()
criterion = FocalLoss()
criterion = criterion.to(device)

#criterion = log_loss()
#logloss = log_loss(y_test, model.predict_proba(X_test))

# Train for 5 epochs
for epoch in range(5):
    loss = train_model(model, train_loader, optimizer, criterion)
    print(f"Epoch {epoch+1} - Loss: {loss:.4f}")

##########################################
# Evaluate on training set (for demonstration)
##########################################
metrics = evaluate_model(model, train_loader)
print("Training Set Evaluation:")
for k, v in metrics.items():
    print(f"{k}: {v:.4f}")

##########################################
#  Cross-validation + multi-seed training loop
##########################################
from sklearn.model_selection import KFold
import random

!date

# Hyperparameters
n_splits = 5
seeds = [42, 123, 2023, 7, 99]
num_epochs = 100 #Change this to 100 for more accuracy? it was 50
batch_size = 512

# Store results
cv_results = []

# Loop over seeds
for seed in seeds:
    print(f"Running for seed {seed}...")
    torch.manual_seed(seed)
    np.random.seed(seed)
    random.seed(seed)

    # Use KFold splitting
    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)
    for fold_idx, (train_idx, test_idx) in enumerate(kf.split(X_train)):
        print(f"  Fold {fold_idx+1}/{n_splits}")
        !date
        # Prepare data loaders
        X_tr, Y_tr, D_tr = X_train[train_idx], Y_train[train_idx], D_train[train_idx]
        X_te, Y_te, D_te = X_train[test_idx], Y_train[test_idx], D_train[test_idx]

        train_loader = torch.utils.data.DataLoader(
            torch.utils.data.TensorDataset(X_tr, Y_tr, D_tr),
            batch_size=batch_size, shuffle=True
        )

        test_loader = torch.utils.data.DataLoader(
            torch.utils.data.TensorDataset(X_te, Y_te, D_te),
            batch_size=batch_size, shuffle=False
        )

        # Model, optimizer, criterion
        model = ContextualAttentionCF(omic_dim=X.shape[1], drug_dim=Y.shape[1])
        model = model.to(device)
        optimizer = optim.Adam(model.parameters(), lr=0.001)
        #criterion = nn.BCELoss()
        criterion = FocalLoss()
        criterion = criterion.to(device)

        # Train
        for epoch in range(num_epochs):
            loss = train_model(model, train_loader, optimizer, criterion)
            print(f"    Epoch {epoch+1}/{num_epochs}, Loss: {loss:.4f}")

        # Evaluate
        metrics = evaluate_model(model, test_loader)
        print(f"    Test F1: {metrics['f1']:.4f}, AUC: {metrics['roc_auc']:.4f}, Accuracy: {metrics['accuracy']:.4f}")

        # Save
        cv_results.append({
            'seed': seed,
            'fold': fold_idx + 1,
            **metrics
        })
        !date
      
#######################################
# Summarize cross-validation results
#######################################
cv_df = pd.DataFrame(cv_results)
print(cv_df.groupby('fold').mean())
print("Overall Mean:")
print(cv_df.mean(numeric_only=True))
print("Overall Std Dev:")
print(cv_df.std(numeric_only=True))

!date
